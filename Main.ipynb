{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da2490fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "def pil_loader(fp: Path, mode: str) -> Image.Image:\n",
    "    with open(fp, \"rb\") as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert(mode)\n",
    "\n",
    "def first_and_last_nonzeros(arr):\n",
    "    for i in range(len(arr)):\n",
    "        if arr[i] != 0:\n",
    "            break\n",
    "    left = i\n",
    "    for i in reversed(range(len(arr))):\n",
    "        if arr[i] != 0:\n",
    "            break\n",
    "    right = i\n",
    "    return left, right\n",
    "\n",
    "\n",
    "def crop(filename: Path, padding: int = 8) -> Optional[Image.Image]:\n",
    "    image = pil_loader(filename, mode=\"RGBA\")\n",
    "\n",
    "    # Replace the transparency layer with a white background\n",
    "    new_image = Image.new(\"RGBA\", image.size, \"WHITE\")\n",
    "    new_image.paste(image, (0, 0), image)\n",
    "    new_image = new_image.convert(\"L\")\n",
    "\n",
    "    # Invert the color to have a black background and white text\n",
    "    arr = 255 - np.array(new_image)\n",
    "\n",
    "    # Area that has text should have nonzero pixel values\n",
    "    row_sums = np.sum(arr, axis=1)\n",
    "    col_sums = np.sum(arr, axis=0)\n",
    "    y_start, y_end = first_and_last_nonzeros(row_sums)\n",
    "    x_start, x_end = first_and_last_nonzeros(col_sums)\n",
    "\n",
    "    # Some images have no text\n",
    "    if y_start >= y_end or x_start >= x_end:\n",
    "        print(f\"{filename.name} is ignored because it does not contain any text\")\n",
    "        return None\n",
    "\n",
    "    # Cropping\n",
    "    cropped = arr[y_start : y_end + 1, x_start : x_end + 1]\n",
    "    H, W = cropped.shape\n",
    "\n",
    "    # Add paddings\n",
    "    new_arr = np.zeros((H + padding * 2, W + padding * 2))\n",
    "    new_arr[padding : H + padding, padding : W + padding] = cropped\n",
    "\n",
    "    # Invert the color back to have a white background and black text\n",
    "    new_arr = 255 - new_arr\n",
    "    return Image.fromarray(new_arr).convert(\"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b268c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "home = Path.home()\n",
    "RAW_IMAGES_DIRNAME = home / \"Downloads/ML_Project/formula_images\"\n",
    "PROCESSED_IMAGES_DIRNAME = home / \"Downloads/ML_Project/formula_images_processed\"\n",
    "\n",
    "class LatexDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, split=\"train\"):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        formula_file = os.path.join(self.data_dir, \"im2latex_formulas.lst\")\n",
    "        image_list_file = os.path.join(self.data_dir, f\"im2latex_{self.split}.lst\")\n",
    "\n",
    "        with open(formula_file, \"r\", encoding=\"latin-1\") as f:\n",
    "            formulas = f.readlines()\n",
    "\n",
    "        with open(image_list_file, \"r\", encoding=\"latin-1\") as f:\n",
    "            image_list = f.readlines()\n",
    "\n",
    "        for line in image_list:\n",
    "            parts = line.strip().split(\" \")\n",
    "            formula_idx = int(parts[0])\n",
    "            image_name = parts[1]\n",
    "            render_type = parts[2]\n",
    "\n",
    "            label = formulas[formula_idx].strip()\n",
    "\n",
    "            image_path = os.path.join(self.data_dir, \"formula_images\", f\"{image_name}.png\")\n",
    "\n",
    "            self.images.append(image_path)\n",
    "            self.labels.append(label)\n",
    "        \n",
    "        if not PROCESSED_IMAGES_DIRNAME.exists():\n",
    "            PROCESSED_IMAGES_DIRNAME.mkdir(parents=True, exist_ok=True)\n",
    "            print(\"Cropping images...\")\n",
    "            for image_filename in RAW_IMAGES_DIRNAME.glob(\"*.png\"):\n",
    "                cropped_image = crop(image_filename, padding=8)\n",
    "                if not cropped_image:\n",
    "                    continue\n",
    "                cropped_image.save(PROCESSED_IMAGES_DIRNAME / image_filename.name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        image = Image.open(image_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return {\"image\": image, \"label\": label}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "data_dir = r\"C:\\Users\\avnee\\Downloads\\ML_Project\\formula_images\"\n",
    "\n",
    "train_dataset = LatexDataset(data_dir, transform=transform, split=\"train\")\n",
    "val_dataset = LatexDataset(data_dir, transform=transform, split=\"validate\")\n",
    "test_dataset = LatexDataset(data_dir, transform=transform, split=\"test\")\n",
    "\n",
    "batch_size = 32\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a751f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class CNNWithPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, height, width):\n",
    "        super(CNNWithPositionalEncoding, self).__init__()\n",
    "\n",
    "        # CNN architecture\n",
    "        self.conv1 = nn.Conv2d(1, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "\n",
    "        self.conv3 = nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        self.conv5 = nn.Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "\n",
    "        self.conv6 = nn.Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoding = positionalencoding2d(d_model, height, width)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        x = F.relu(self.conv6(x))\n",
    "\n",
    "        # Add positional encoding\n",
    "        x = x + self.positional_encoding.unsqueeze(0)  # Broadcasting the positional encoding\n",
    "\n",
    "        return x\n",
    "\n",
    "def positionalencoding2d(d_model, height, width):\n",
    "    \"\"\"\n",
    "    :param d_model: dimension of the model\n",
    "    :param height: height of the positions\n",
    "    :param width: width of the positions\n",
    "    :return: d_model*height*width position matrix\n",
    "    \"\"\"\n",
    "    if d_model % 4 != 0:\n",
    "        raise ValueError(\"Cannot use sin/cos positional encoding with \"\n",
    "                         \"odd dimension (got dim={:d})\".format(d_model))\n",
    "    pe = torch.zeros(d_model, height, width)\n",
    "    # Each dimension use half of d_model\n",
    "    d_model = int(d_model / 2)\n",
    "    div_term = torch.exp(torch.arange(0., d_model, 2) *\n",
    "                         -(math.log(10000.0) / d_model))\n",
    "    pos_w = torch.arange(0., width).unsqueeze(1)\n",
    "    pos_h = torch.arange(0., height).unsqueeze(1)\n",
    "    pe[0:d_model:2, :, :] = torch.sin(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
    "    pe[1:d_model:2, :, :] = torch.cos(pos_w * div_term).transpose(0, 1).unsqueeze(1).repeat(1, height, 1)\n",
    "    pe[d_model::2, :, :] = torch.sin(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "    pe[d_model + 1::2, :, :] = torch.cos(pos_h * div_term).transpose(0, 1).unsqueeze(2).repeat(1, 1, width)\n",
    "\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daeb73da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import json\n",
    "with open(\"vocab.json\", \"r\") as f:\n",
    "    VOCAB = json.load(f)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden):\n",
    "        batch_size, seq_len, _ = encoder_outputs.shape\n",
    "\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, attention):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.attention = attention\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.embedding(input)\n",
    "        a = self.attention(encoder_outputs, hidden[0])\n",
    "        a = a.unsqueeze(1)\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        output = torch.cat((embedded.squeeze(0), hidden[0].squeeze(0), weighted.squeeze(1)), dim=1)\n",
    "        prediction = self.fc_out(output)\n",
    "        return prediction, hidden\n",
    "\n",
    "class Seq2SeqWithAttention(nn.Module):\n",
    "    def __init__(self, cnn_model, attention, output_dim, emb_dim, enc_hid_dim, dec_hid_dim):\n",
    "        super(Seq2SeqWithAttention, self).__init__()\n",
    "\n",
    "        self.cnn_model = cnn_model\n",
    "        self.attention = attention\n",
    "        self.decoder = Decoder(output_dim, emb_dim, enc_hid_dim, dec_hid_dim, attention)\n",
    "\n",
    "    def forward(self, x, target, teacher_forcing_ratio=0.5):\n",
    "        encoder_outputs = self.cnn_model(x)\n",
    "\n",
    "        batch_size, _, _ = encoder_outputs.shape\n",
    "        trg_len = target.shape[1]\n",
    "        trg_vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(x.device)\n",
    "        input = target[:, 0]\n",
    "\n",
    "        hidden = (torch.zeros(1, batch_size, self.decoder.rnn.hidden_size).to(x.device),\n",
    "                  torch.zeros(1, batch_size, self.decoder.rnn.hidden_size).to(x.device))\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = target[:, t] if teacher_force and t < trg_len - 1 else top1\n",
    "\n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
